services:
  ray-head:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ray-head
    ports:
      - "8000:8000"  # FastAPI backend
      - "8501:8501"  # Streamlit frontend
      - "8265:8265"  # Ray dashboard
      - "6379:6379"  # Ray head port
    environment:
      - RAY_DISABLE_IMPORT_WARNING=1
      - STREAMLIT_SERVER_PORT=8501
    shm_size: '1.5gb' # memoria compartida suficiente para Ray
    mem_limit: 2g     # lÃ­mite RAM contenedor head
  
    command: >
      sh -c "
        echo 'Starting Ray head node...' &&
        ray start --head --port=6379 --disable-usage-stats --dashboard-host=0.0.0.0 --dashboard-port=8265 --object-store-memory=300000000 --num-cpus=2 --memory=1500000000 &&
        echo 'Ray head started, waiting for full initialization...' &&
        sleep 120 &&
        echo 'Starting FastAPI and Streamlit...' &&
        uvicorn inicio:app --host 0.0.0.0 --port 8000 --reload &
        streamlit run streamlit_fronted.py --server.port=8501 --server.address=0.0.0.0 --server.headless=true
      "
    networks:
      - ml-net

  ray_worker:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - ray-head
    environment:
      - RAY_DISABLE_IMPORT_WARNING=1
    shm_size: '1gb'
    mem_limit: 1.2g
    command: >
      sh -c "
        echo 'Waiting for Ray head to be ready...' &&
        sleep 200 &&
        for i in {1..30}; do
          if ray status --address=ray-head:6379 >/dev/null 2>&1; then
            echo 'Ray head is ready, starting worker...'
            break
          else
            echo 'Ray head not ready yet, waiting...' 
            sleep 5
          fi
        done &&
        ray start --address=ray-head:6379 --object-store-memory=200000000 --memory=1000000000 &&
        echo 'Ray worker started successfully' &&
        tail -f /dev/null
      "
    networks:
      - ml-net
    scale: 2  # Start with 2 workers by default

networks:
  ml-net:
    driver: bridge